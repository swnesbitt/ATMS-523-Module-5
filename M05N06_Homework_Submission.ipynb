{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4d2eb8",
   "metadata": {},
   "source": [
    "\n",
    "# Module 5 Project: Radar Parameters and Rainfall Prediction\n",
    "### By: Nathan Makowski (Student in ATMS 523 Fall 2025)\n",
    "\n",
    "\n",
    "In the **Module 5 Project** we analyze polarimetric radar data to estimate rainfall intensity. Using measurements collected from disdrometers in **Huntsville, Alabama**, this notebook explores how various radar-derived parameters relate to rain rate.\n",
    "\n",
    "\n",
    "In this project, I will:\n",
    "\n",
    "\n",
    "1. Load and explore the dataset `radar_parameters.csv` (8 columns, 18,969 rows).\n",
    "2. Split the dataset into **70% training** and **30% testing** subsets.\n",
    "3. Train and evaluate multiple regression models, including:\n",
    "- **Multiple Linear Regression**\n",
    "- **Polynomial Regression** (with grid search over polynomial degree 0–9, using 7-fold cross-validation)\n",
    "- **Random Forest Regressor** (with grid search over a specified hyperparameter grid)\n",
    "4. Compare each model’s performance to a **baseline empirical relationship** between radar reflectivity and rain rate:\n",
    "\\( Z = 200R^{1.6} \\)\n",
    "\n",
    "\n",
    "Performance metrics used include:\n",
    "- **Coefficient of Determination (R²)** — measures explained variance.\n",
    "- **Root Mean Square Error (RMSE)** — measures average prediction error.\n",
    "\n",
    "\n",
    "By the end of this analysis, I identify which model most accurately estimates rainfall rate and whether it can outperform the baseline relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d520b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdb8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/njm12/ATMS_523/Module_5/ATMS-523-Module-5\n"
     ]
    }
   ],
   "source": [
    "# If you want to quickly check your current working directory in the notebook, uncomment:\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe0a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows, cols: (18969, 8)\n",
      "['Unnamed: 0', 'Zh (dBZ)', 'Zdr (dB)', 'Ldr (dB)', 'Kdp (deg km-1)', 'Ah (dBZ/km)', 'Adr (dB/km)', 'R (mm/hr)']\n"
     ]
    }
   ],
   "source": [
    "# 0) Load data (adjust path if necessary)\n",
    "csv_path = '/home/njm12/ATMS_523/Module_5/ATMS-523-Module-5/homework/radar_parameters.csv'\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Could not find {csv_path}. If your notebook's working directory is not the repo root, either change csv_path or run: import os; os.getcwd() to inspect the cwd.\")\n",
    "\n",
    "\n",
    "# Quick peek\n",
    "print('rows, cols:', df.shape)\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb2ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "feature_cols = ['Zh (dBZ)', 'Zdr (dB)', 'Ldr (dB)', 'Kdp (deg km-1)', 'Ah (dBZ/km)', 'Adr (dB/km)']\n",
    "# Drop any unnamed index column if present\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "expected_cols = feature_cols + ['R (mm/hr)']\n",
    "if not all(col in df.columns for col in expected_cols):\n",
    "    raise ValueError('Input CSV missing expected columns. Expected columns include: ' + ','.join(expected_cols))\n",
    "\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df['R (mm/hr)'].values\n",
    "\n",
    "#print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475084fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 13278 Test size: 5691\n",
      "\n",
      "Baseline (Z=200 R^1.6) metrics:\n",
      " Train R2: 0.2756, RMSE: 7.1440\n",
      " Test R2: 0.3566, RMSE: 7.1893\n"
     ]
    }
   ],
   "source": [
    "# 1) Split 70/30 (train/test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "print('Train size:', X_train.shape[0], 'Test size:', X_test.shape[0])\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# Baseline function: Z = 200 R^1.6 --> R = (Z/200)^(1/1.6)\n",
    "# Note: Zh is in dBZ, Z = 10^(Zh/10)\n",
    "\n",
    "\n",
    "def baseline_R_from_Zh(Zh_array):\n",
    "    Z = 10**(Zh_array / 10.0)\n",
    "    R_hat = (Z / 200.0) ** (1.0 / 1.6)\n",
    "    return R_hat\n",
    "\n",
    "\n",
    "# Compute baseline on train and test\n",
    "R_baseline_train = baseline_R_from_Zh(X_train[:, 0])\n",
    "R_baseline_test = baseline_R_from_Zh(X_test[:, 0])\n",
    "\n",
    "\n",
    "# Baseline metrics\n",
    "baseline_train_r2 = r2_score(y_train, R_baseline_train)\n",
    "baseline_test_r2 = r2_score(y_test, R_baseline_test)\n",
    "baseline_train_rmse = rmse(y_train, R_baseline_train)\n",
    "baseline_test_rmse = rmse(y_test, R_baseline_test)\n",
    "\n",
    "\n",
    "print('\\nBaseline (Z=200 R^1.6) metrics:')\n",
    "print(f' Train R2: {baseline_train_r2:.4f}, RMSE: {baseline_train_rmse:.4f}')\n",
    "print(f' Test R2: {baseline_test_r2:.4f}, RMSE: {baseline_test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba0b354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.44801283780265 80.33838182548162\n"
     ]
    }
   ],
   "source": [
    "print(np.var(y_train), np.var(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ea0b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression metrics:\n",
      " Train R2: 0.9879, RMSE: 0.9229\n",
      " Test R2: 0.9891, RMSE: 0.9358\n"
     ]
    }
   ],
   "source": [
    "# 2) Multiple Linear Regression (using all features)\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = pipe_lr.predict(X_train)\n",
    "y_test_pred_lr = pipe_lr.predict(X_test)\n",
    "\n",
    "\n",
    "lr_train_r2 = r2_score(y_train, y_train_pred_lr)\n",
    "lr_test_r2 = r2_score(y_test, y_test_pred_lr)\n",
    "lr_train_rmse = rmse(y_train, y_train_pred_lr)\n",
    "lr_test_rmse = rmse(y_test, y_test_pred_lr)\n",
    "\n",
    "\n",
    "print('\\nLinear Regression metrics:')\n",
    "print(f' Train R2: {lr_train_r2:.4f}, RMSE: {lr_train_rmse:.4f}')\n",
    "print(f' Test R2: {lr_test_r2:.4f}, RMSE: {lr_test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd4744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 10 candidates, totalling 70 fits\n",
      "\n",
      "Polynomial GridSearch best params: {'poly__degree': 2}\n",
      "Best CV R2: 0.9969985736490088\n",
      "\n",
      "Best Polynomial model metrics:\n",
      " degree = 2\n",
      " Train R2: 0.9996, RMSE: 0.1672\n",
      " Test R2: 0.9996, RMSE: 0.1836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:516: FitFailedWarning: \n",
      "7 fits failed out of a total of 70.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/base.py\", line 897, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/preprocessing/_polynomial.py\", line 313, in fit\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Setting degree to zero and include_bias to False would result in an empty output array.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [            nan  9.85456070e-01  9.96998574e-01  6.14352327e-01\n",
      " -4.17284835e+02 -4.97398175e+05 -1.54662552e+08 -2.93229059e+09\n",
      " -2.86848086e+11 -1.91271590e+16]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3) Polynomial features grid search (degrees 0-9) with 7-fold CV\n",
    "# We'll create a pipeline: scaler -> PolynomialFeatures(degree=d, include_bias=False) -> LinearRegression\n",
    "# Note: degree=0 means only bias term (constant model). We include include_bias=False then allow LinearRegression intercept.\n",
    "\n",
    "\n",
    "pipeline_poly = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('poly', PolynomialFeatures(include_bias=False)),\n",
    "('linreg', LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "'poly__degree': list(range(0, 10))\n",
    "}\n",
    "\n",
    "\n",
    "grid_poly = GridSearchCV(pipeline_poly, param_grid, cv=7, scoring='r2', n_jobs=-1, verbose=1)\n",
    "grid_poly.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('\\nPolynomial GridSearch best params:', grid_poly.best_params_)\n",
    "print('Best CV R2:', grid_poly.best_score_)\n",
    "\n",
    "\n",
    "best_poly = grid_poly.best_estimator_\n",
    "# Evaluate best on train and test\n",
    "y_train_pred_poly = best_poly.predict(X_train)\n",
    "y_test_pred_poly = best_poly.predict(X_test)\n",
    "\n",
    "\n",
    "poly_train_r2 = r2_score(y_train, y_train_pred_poly)\n",
    "poly_test_r2 = r2_score(y_test, y_test_pred_poly)\n",
    "poly_train_rmse = rmse(y_train, y_train_pred_poly)\n",
    "poly_test_rmse = rmse(y_test, y_test_pred_poly)\n",
    "\n",
    "\n",
    "print('\\nBest Polynomial model metrics:')\n",
    "print(f\" degree = {grid_poly.best_params_['poly__degree']}\")\n",
    "print(f' Train R2: {poly_train_r2:.4f}, RMSE: {poly_train_rmse:.4f}')\n",
    "print(f' Test R2: {poly_test_r2:.4f}, RMSE: {poly_test_rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7653754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 64 candidates, totalling 448 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njm12/ATMS_523/envs/xarray-climate/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest GridSearch best params: {'bootstrap': True, 'max_depth': 100, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best CV R2: 0.9813631013370073\n",
      "\n",
      "Best Random Forest metrics:\n",
      " Train R2: 0.9974, RMSE: 0.4252\n",
      " Test R2: 0.9879, RMSE: 0.9858\n"
     ]
    }
   ],
   "source": [
    "# 4) Random Forest Regressor grid search (use provided param_grid), 7-fold CV\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "param_grid_rf = {\n",
    "\"bootstrap\": [True, False],\n",
    "\"max_depth\": [10, 100],\n",
    "\"max_features\": [\"sqrt\", 1.0],\n",
    "\"min_samples_leaf\": [1, 4],\n",
    "\"min_samples_split\": [2, 10],\n",
    "\"n_estimators\": [200, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=7, scoring='r2', n_jobs=-1, verbose=1)\n",
    "# Warning: this search can be computationally heavy depending on data size and your machine.\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('\\nRandom Forest GridSearch best params:', grid_rf.best_params_)\n",
    "print('Best CV R2:', grid_rf.best_score_)\n",
    "\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "# Evaluate best on train and test\n",
    "y_train_pred_rf = best_rf.predict(X_train)\n",
    "y_test_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "\n",
    "rf_train_r2 = r2_score(y_train, y_train_pred_rf)\n",
    "rf_test_r2 = r2_score(y_test, y_test_pred_rf)\n",
    "rf_train_rmse = rmse(y_train, y_train_pred_rf)\n",
    "rf_test_rmse = rmse(y_test, y_test_pred_rf)\n",
    "\n",
    "\n",
    "print('\\nBest Random Forest metrics:')\n",
    "print(f' Train R2: {rf_train_r2:.4f}, RMSE: {rf_train_rmse:.4f}')\n",
    "print(f' Test R2: {rf_test_r2:.4f}, RMSE: {rf_test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4b88b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of models:\n",
      "                 model  train_r2   test_r2  train_rmse  test_rmse\n",
      "0      Baseline (Z->R)  0.275551  0.356643    7.143950   7.189316\n",
      "1     LinearRegression  0.987909  0.989099    0.922940   0.935812\n",
      "2           Poly deg=2  0.999603  0.999581    0.167173   0.183564\n",
      "3  RandomForest (best)  0.997433  0.987904    0.425229   0.985775\n",
      "\n",
      "Saved best models to disk: best_linear_model.joblib, best_polynomial_model.joblib, best_random_forest_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# 5) Summary comparison table\n",
    "results = pd.DataFrame({\n",
    "'model': ['Baseline (Z->R)', 'LinearRegression', f'Poly deg={grid_poly.best_params_[\"poly__degree\"]}', 'RandomForest (best)'],\n",
    "'train_r2': [baseline_train_r2, lr_train_r2, poly_train_r2, rf_train_r2],\n",
    "'test_r2': [baseline_test_r2, lr_test_r2, poly_test_r2, rf_test_r2],\n",
    "'train_rmse': [baseline_train_rmse, lr_train_rmse, poly_train_rmse, rf_train_rmse],\n",
    "'test_rmse': [baseline_test_rmse, lr_test_rmse, poly_test_rmse, rf_test_rmse]\n",
    "})\n",
    "\n",
    "\n",
    "print('\\nComparison of models:')\n",
    "print(results)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Save the best models if you want\n",
    "joblib.dump(pipe_lr, 'best_linear_model.joblib')\n",
    "joblib.dump(best_poly, 'best_polynomial_model.joblib')\n",
    "joblib.dump(best_rf, 'best_random_forest_model.joblib')\n",
    "\n",
    "\n",
    "print('\\nSaved best models to disk: best_linear_model.joblib, best_polynomial_model.joblib, best_random_forest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f4e1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model file sizes:\n",
      " best_linear_model.joblib: 0.00 MB\n",
      " best_polynomial_model.joblib: 0.00 MB\n",
      " best_random_forest_model.joblib: 230.65 MB\n"
     ]
    }
   ],
   "source": [
    "# Print file sizes\n",
    "print('\\nSaved model file sizes:')\n",
    "for file in ['best_linear_model.joblib', 'best_polynomial_model.joblib', 'best_random_forest_model.joblib']:\n",
    "    size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "    print(f\" {file}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54facb8",
   "metadata": {},
   "source": [
    "# Model Performance Comparison and Discussion\n",
    "\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "All three machine learning models (**Linear**, **Polynomial**, **Random Forest**) dramatically outperform the baseline \\( Z = 200R^{1.6} \\) model.\n",
    "\n",
    "\n",
    "- The **baseline’s test R² ≈ 0.36** shows it explains only ~36% of the variance, whereas the others explain >98%.\n",
    "- Its **RMSE (~7.19 mm/hr)** is an order of magnitude larger than the ML models (**~0.18–0.98 mm/hr**).\n",
    "- **Linear regression** already performs extremely well (**R² ≈ 0.99**, RMSE ≈ 0.94 mm/hr).\n",
    "- **Polynomial regression (degree = 2)** provides a small but meaningful improvement over linear:\n",
    "- R² rises to **0.9996**\n",
    "- RMSE drops to **0.18 mm/hr**, indicating an almost perfect fit without overfitting (train and test scores nearly identical).\n",
    "- **Random Forest** achieves excellent training performance but does not beat the polynomial model on the test set:\n",
    "- Test R² ≈ **0.988** (slightly worse than linear/polynomial)\n",
    "- Test RMSE ≈ **0.986 mm/hr** (worse than polynomial, similar to linear)\n",
    "\n",
    "\n",
    " **Therefore:**\n",
    "- The **best polynomial model (degree = 2)** outperforms the baseline, linear regression, and optimized Random Forest models in both **R²** and **RMSE**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
